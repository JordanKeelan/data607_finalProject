{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHzGJrNxSzE9"
      },
      "source": [
        "# Time Series Analysis of Spanish Wine Production \n",
        "\n",
        "**Arthur Trim, Kane Smith, Jordan Keelan, Rodrigo Rosales Alvarez, Scott Bennett**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9DZYkjY0SzFA"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import os\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "from datetime import datetime\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.losses import huber_loss\n",
        "\n",
        "# Set seaborn theme for plots\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "\n",
        "# Ignore warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQQXcWvHSzFA"
      },
      "source": [
        "## 1: Introduction \n",
        "\n",
        "### 1.1: Background and Context  \n",
        "\n",
        "Agricultural production is the foundation of the global economy. Globally it provides jobs to over 1 billion people (Map of the month 2020) and provides food security to billions. Without agriculture, there would be no way to support our current population numbers and density. Without affordable and efficient access to food, economies world collapse. The importance of agricultural production cannot be overstated. For this project we are focused on Spanish production.  \n",
        "\n",
        "Wine has always been an important beverage in society, not only for its taste and flavor; economically, wine production is a significant industry that generates billions of dollars in revenue each year. Moreover, wine is highly related with tourism, as people will travel to wineries to experience the production process and taste different varieties of wine, this has a huge positive impact on local economies, as tourism can create jobs and generate revenue for local businesses. \n",
        "\n",
        "Socially, wine plays an important role as it often accompanies celebrations and special occasions. Wine brings people together, creating and strengthening relationships.  \n",
        "\n",
        "Spain is one of the most important wine-producing countries in the world and according to the International Organization of Vine and Wine (OIV), is among the top three wine-producing countries along with Italy and France. In 2020, Spain produced 1,310,174 tons of wine, accounting for 16% of the world's total wine production. \n",
        "\n",
        "As population continues to grow, so must food production. As we enter an era of increased variance in global temperatures, weather patterns, and natural disasters understanding the nature of food production trends will be necessary in assessing the health of our agricultural industries. Forecasting can help to inform law/policy makers, it can help to optimize resource allocation, and inform supply chains. \n",
        "\n",
        "Time series analysis is a statistical and machine learning technique used to model trends and forecast future values of temporal data. This type of analysis will be effective in understanding, describing, and predicting trends in agricultural data production. Time series analysis techniques such as Autoregressive Integrated Moving Average (ARIMA), Seasonal ARIMA (SARIMA), Long Short-Term Memory (LSTM) networks, and Facebook Prophet, can be employed to model and predict crop production trends. Each of these methods offers unique advantages and limitations, depending on the underlying data structure, model complexity, and forecasting requirements. \n",
        "\n",
        "<br>\n",
        "\n",
        "### 1.2: Objectives of the Study  \n",
        "\n",
        "This study aims to apply different time series analysis techniques to the FAOSTAT production indices dataset, obtained from the Food and Agriculture Organization of the United Nations (FAO), to forecast crop production trends. The results will provide insights into the most appropriate time series forecasting method for predicting crop production trends and offer valuable input for decision-making in agricultural planning and management. \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrRSObHfSzFB"
      },
      "source": [
        "## 2: Project Dataset \n",
        "\n",
        "### 2.1 FAOSTAT Crops and Livestock Products Dataset \n",
        "\n",
        "The Food and Agriculture Organization (FAO) is a United Nations agency dedicated to leading international efforts to combat hunger. FAO's mission is to ensure food security for all, enabling individuals to have consistent access to sufficient, high-quality food to maintain active and healthy lives. \n",
        "\n",
        "The FAOSTAT data portal offers a diverse range of data on subjects such as food production, food security, trade, climate change, and other relevant indicators. In this study, we will concentrate on the \"Crops and Livestock Products\" dataset, which encompasses variables like country, year, and specific crop production data.  \n",
        "\n",
        "<br>\n",
        "\n",
        "### 2.2 Data Collection Process \n",
        "\n",
        "The FAOSTAT portal provides a user-friendly interface for data extraction, allowing both bulk data downloads and pre-filtered selections based on specific parameters. For this study, we will utilize the pre-filtering option to obtain data specific to our target crop, location, and time series for modeling purposes. The extracted data will be saved in CSV format, facilitating further manipulation and modeling using Python3. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dIwZcD5-SzFC"
      },
      "outputs": [],
      "source": [
        "# parent directory\n",
        "parent = os.path.dirname(os.getcwd())\n",
        "\n",
        "# load main csv\n",
        "wine_url = \"https://raw.githubusercontent.com/JordanKeelan/data607_finalProject/main/faostat_wine.csv\"\n",
        "wine = pd.read_csv(wine_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u7RCBL2SzFC"
      },
      "source": [
        "### 2.3: Data Pre-processing \n",
        "\n",
        "Before starting our analysis, we cleaned up the CSV data from the FAOSTAT portal with the following steps: \n",
        "- Reviewed the columns in the data table to check for irrelevant columns. \n",
        "- Used python to check for the unique values for units in the dataset and confirmed that the wine production data is all reported in tons, with no other units appearing. \n",
        "- Converted the ‘Year’ text values into Datetime format. \n",
        "- Removed any extraneous records from the table by discarding any rows with null data in the ‘Value’ column. \n",
        "- Dropped unnecessary columns so only the Area, Year, and Value columns remain. \n",
        "- Indexed the Year column in the data tables. \n",
        "\n",
        "With these steps complete, we obtain cleaned data tables with annual wine production and temperature variation for our selected countries (Italy, France, Spain, and the USA)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN1-CkrvSzFC",
        "outputId": "1d29e855-0abd-4d68-9768-c5c58c967334"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tonnes'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Confirm only one unit in dataset\n",
        "set(wine[\"Unit\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4CTFV0gSzFD"
      },
      "outputs": [],
      "source": [
        "# Set datatype of Year column to datetime\n",
        "wine[\"Year Code\"] = pd.to_datetime(wine[\"Year Code\"],format=\"%Y\")\n",
        "\n",
        "# Remove rows with null values in \"value\" column\n",
        "wine = wine[wine[\"Value\"].notna()]\n",
        "\n",
        "# Remove unnecessary columns\n",
        "wine = wine[[\"Area\",\"Year Code\",\"Value\"]]\n",
        "\n",
        "# Datetime index dataframes\n",
        "wine = wine.set_index(\"Year Code\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO848xrhSzFE"
      },
      "source": [
        "## 3: Exploratory Data Analysis (EDA)  \n",
        "\n",
        "### 3.1: Visualizations and Trends Analysis  \n",
        "\n",
        "<br>\n",
        "\n",
        "### 3.2: Seasonality and Patterns  \n",
        "\n",
        "<br>\n",
        "\n",
        "### 3.3: Unusual Observations and Data Issues \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lkUNDFFSzFE"
      },
      "source": [
        "## 4: Time Series Decomposition  \n",
        "\n",
        "### 4.1 Trend Component  \n",
        "\n",
        "<br>\n",
        "\n",
        "### 4.2: Seasonal Component  \n",
        "\n",
        "<br>\n",
        "\n",
        "### 4.3: Residual Component \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPejhwCNSzFE"
      },
      "source": [
        "## 5: Model Development and Evaluation  \n",
        "\n",
        "### 5.1: Data Splitting: Training and Testing Sets  \n",
        "\n",
        "<br>\n",
        "\n",
        "### 5.2: Forecasting Models  \n",
        "\n",
        "<br>\n",
        "\n",
        "**5.2.1: ARIMA**  \n",
        "\n",
        "**5.2.2: SARIMA**  \n",
        "\n",
        "**5.2.3: LSTM**  \n",
        "\n",
        "**5.2.4: Facebook Prophet**\n",
        "\n",
        "<br>\n",
        "\n",
        "### 5.3: Model Evaluation Criteria  \n",
        "\n",
        "<br>\n",
        "\n",
        "**5.3.1: Mean Absolute Error (MAE)**\n",
        "\n",
        "**5.3.2: Root Mean Squared Error (RMSE)**\n",
        "\n",
        "**5.3.3: Model Simplicity and Interpretability**\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECPFcZLSSzFE"
      },
      "source": [
        "## 6: Model Implementation  \n",
        "\n",
        "### 6.1: ARIMA Model  \n",
        "\n",
        "Autoregressive Integrated Moving Average is a type of time series model used to analyze and forecast future values of a time series based on its past behavior, or lagged values. It is important to stablish that this model does not consider seasonality of the data.  \n",
        "\n",
        "The ARIMA model is composed of three components:  \n",
        "- Autoregressive Component: represents the correlation between the current value of the time series and its past values. \n",
        "- Integrated Component: represents the differencing of the time series to make it stationary. \n",
        "- Moving Average Component: represents the correlation between the current value and the errors of past predictions. \n",
        "\n",
        "ARIMA models are widely used in different industries like finance, economics, and other fields where forecasting is important; also, is widely used for outlier detection.  \n",
        "\n",
        "The most important assumption of the ARIMA model is that the data is stationary, in other words that the mean and variance of the data remains constant over time. To prove it, we used the Augmented Dickey-Fuller test, this test has a Null Hypothesis that the data is non-stationary. We obtained a p_value of 0.00007, meaning that we can reject the Null, and state that our data presents stationarity.  \n",
        "\n",
        "To implement the ARIMA model in python we used the *ARIMA* class present in the *statsmodels.tsa.arima.model* module as it provides a convenient way to fit an ARIMA model into a time series analysis and offers the possibility to forecast and evaluate the model.  \n",
        "\n",
        "**6.1.2: Parameter Selection**\n",
        "\n",
        "Three parameters that are important to build an ARIMA model are: \n",
        "- P: represents the number of lagged values of the time series that are included in the model. \n",
        "- Q: represents the number of lagged errors of the time series that are included in the model. \n",
        "- D: represents the number of times that the time series is differenced to achieve stationarity. \n",
        "\n",
        "Grid Search is a technique used for hyperparameter tuning in machine learning models, in other words is a method for systematically testing different combinations of hyperparameters to identify the best set of parameters for a given model. We found that the best parameters for our model are p=4, q=3, d=2. \n",
        "\n",
        "**6.1.2: Model Fitting**  \n",
        "\n",
        "To train our model, we used annually wine production in Spain from 1961 to 2010 and used data from 2011 to 2020 to test our model. Please check the Appendix A, section ARIMA to review the code we implemented.  \n",
        "\n",
        "Here below, we can observe the real production of wine in Spain in pink and the predicted value from our ARIMA model in blue. As we can see, our prediction follows the general trend of the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF_YKOflSzFF"
      },
      "outputs": [],
      "source": [
        "# Grid Search ARIMA\n",
        "country = 'Spain'\n",
        "value = 'Value'\n",
        "\n",
        "# Prepare the training and testing data\n",
        "train = wine[(wine['Area'] == country)].iloc[0:50]\n",
        "test = wine[(wine['Area'] == country)].iloc[50:]\n",
        "\n",
        "# Define the range of p, d, q, P, D, and Q values\n",
        "p = range(0, 6)\n",
        "d = range(0, 3)\n",
        "q = range(0, 6)\n",
        "\n",
        "# Generate all possible combinations of p, d, q\n",
        "parameter_combinations = list(itertools.product(p, d, q))\n",
        "\n",
        "# Initialize the best model parameters and the lowest MAE\n",
        "best_parameters = None\n",
        "lowest_mae = float(\"inf\")\n",
        "\n",
        "# Train and evaluate models for each combination\n",
        "for parameters in parameter_combinations:\n",
        "    p, d, q = parameters\n",
        "    try:\n",
        "        # Fit the SARIMAX model\n",
        "        model = ARIMA(train[value], order=(p, d, q))\n",
        "        model_fit = model.fit()\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = model_fit.predict(start=test[value].index[0], end=test[value].index[-1], typ='levels')\n",
        "\n",
        "        # Calculate the Mean Absolute Error\n",
        "        mae = mean_absolute_error(test[value], predictions)\n",
        "\n",
        "        # Update the best parameters if the current MAE is lower than the previous lowest MAE\n",
        "        if mae < lowest_mae:\n",
        "            lowest_mae = mae\n",
        "            best_parameters = parameters\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "# Print the best parameters and the lowest MAE\n",
        "print(f\"Best parameters: {best_parameters}\")\n",
        "print(f\"Lowest MAE: {lowest_mae:.2f}\")\n",
        "\n",
        "# Plot best parameters\n",
        "arima_model = ARIMA(train[value], order = best_parameters) \n",
        "arima_model_fit = arima_model.fit()\n",
        "predicted_values = arima_model_fit.predict(start=test[value].index[0], end=test[value].index[-1], typ='levels')\n",
        "output = pd.concat([predicted_values, test[value]], axis=1, join=\"inner\")\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "real = plt.plot(wine[(wine['Area'] == country)][value], color='pink',label='Real')\n",
        "predicted = plt.plot(output.iloc[:,0], color='b', label='Predicted', linestyle='--')\n",
        "plt.title('Real vs Predicted')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgBfiKyFSzFF"
      },
      "source": [
        "**6.1.3: Performance Evaluation**\n",
        "\n",
        "Mean Absolute Error (MAE) is used to measure the accuracy of time series forecasting models. It measures the average absolute difference between the predicted and actual values over a given period; a lower value of MAE is desired, which would mean that the model has better accuracy for time series forecasting. For our ARIMA model we got a MAE of 362,190.09 tones of wine.  \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8S6s0R5SzFF"
      },
      "source": [
        "### 6.2: SARIMA Model  \n",
        "\n",
        "**6.2.1: Seasonal Decomposition**\n",
        "\n",
        "**6.2.2: Parameter Selection**  \n",
        "\n",
        "**6.2.3: Model Fitting**  \n",
        "\n",
        "**6.2.4: Performance Evaluation**  \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxHjGg8sSzFF"
      },
      "source": [
        "### 6.3: LSTM Networks  \n",
        "\n",
        "We wanted to create a proof-of-concept neural network for our time series analysis to show ourselves how it would be implemented. Due to the simplicity of it's structure, we did not expect it to outperform some of our other models.\n",
        "\n",
        "LSTM stands for Long Short-Term Memory and is a type of recurent neural network that can be used for time series analysis. The LSTM can take a sequence of previous observations and then predict the next value of the sequence. One of the benefits of using a neural network for time series analysis is it can learn compelx patterns in the data if you have enough data and you tune it correctly; the downside is that it can computationally expensive. Because of this, we we only made a simple LSTM and only searched a small hyperparameter space for tuning the model.\n",
        "\n",
        "**6.3.1: Hyperparameter Selection**\n",
        "\n",
        "In TensorFlow's implementation of LSTM, there are many parameters we can tune to try to improve the model. \n",
        "\n",
        "- Epochs: The number of times the dataset is presented to the model during training. The more epochs, the more computation it will take to train.\n",
        "- Batch size: The number of training examples used in one iteration of the training process.\n",
        "- Neurons: The number of neurons in a layer of the LSTM.\n",
        "- Dropout rate: A regularization technique that randomly drops out some neurons from the model to avoid overfitting.\n",
        "- Learning rate: The magnitude of updates made to model in resonse to the loss function output.\n",
        "- Optimizer: The optimization algorithm used to minimize the loss function.\n",
        "\n",
        "To keep things simple since we did not have a lot of time to run our model, we set epochs to 100, batch size to 1, dropout rate to, neurons to 300 and the optimization algorithm to 'adagrad'. We then used GridSearch to find the best combination in the paramter space of learning_rate = [0.001, 0.01, 0.1].\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al3IzpV5SzFF"
      },
      "source": [
        "**6.3.2: Data Pre-processing** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLRnv_QvSzFF"
      },
      "outputs": [],
      "source": [
        "spain_df = wine[wine[\"Area\"] == \"Spain\"]\n",
        "df = spain_df.drop([\"Unnamed: 0\", \"Value_log\", \"Value Transform\", \"Area\"], axis = 1).reset_index(drop=False)\n",
        "data = df[['Value']].values\n",
        "\n",
        "# normalize data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# split data into train and test sets\n",
        "train_size = len(data) - 14\n",
        "test_size = len(data) - train_size\n",
        "train, test = data[0:train_size,:], data[train_size:len(data),:]\n",
        "\n",
        "# create function to convert data into time series dataset\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + look_back, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "# convert data into time series dataset\n",
        "look_back = 3\n",
        "X_train, Y_train = create_dataset(train, look_back)\n",
        "X_test, Y_test = create_dataset(test, look_back)\n",
        "\n",
        "# reshape data for LSTM input\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bSBHS74SzFG"
      },
      "source": [
        "**6.3.2: Model Training**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgBkEfU6SzFG"
      },
      "outputs": [],
      "source": [
        "# create LSTM model\n",
        "def create_model(lr=0.001):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(300, dropout=0.1, input_shape=(1, look_back)))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adagrad')\n",
        "    return model\n",
        "\n",
        "# create KerasRegressor\n",
        "model = KerasRegressor(build_fn=create_model)\n",
        "\n",
        "# define hyperparameters to search over\n",
        "params = {'lr': [0.001, 0.01, 0.1]}\n",
        "\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=params, scoring='neg_mean_squared_error', cv=3, verbose=2, n_jobs=-1)\n",
        "\n",
        "# fit the grid search\n",
        "grid_result = grid.fit(X_train, Y_train, batch_size=1, verbose=0)\n",
        "\n",
        "# print the best results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "# retrain the model on the best hyperparameters\n",
        "best_lr = grid_result.best_params_['lr']\n",
        "\n",
        "model = create_model(lr=best_lr)\n",
        "model.fit(X_train, Y_train, epochs=100, batch_size=1, verbose=2)\n",
        "\n",
        "# make predictions\n",
        "train_predict = model.predict(X_train)\n",
        "test_predict = model.predict(X_test)\n",
        "\n",
        "# invert predictions back to original scale\n",
        "train_predict = scaler.inverse_transform(train_predict)\n",
        "Y_train = scaler.inverse_transform([Y_train])\n",
        "test_predict = scaler.inverse_transform(test_predict)\n",
        "Y_test = scaler.inverse_transform([Y_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SExbCKgySzFG"
      },
      "source": [
        "**6.3.3: Performance Evaluation** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVWAw02YSzFG"
      },
      "outputs": [],
      "source": [
        "train_score = np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0]))\n",
        "test_score =mean_squared_error(Y_test[0], test_predict[:,0])\n",
        "print(\"Test MAE: \", test_score)\n",
        "\n",
        "\n",
        "dates = [\"2011-01-01\", \"2012-01-01\", \"2013-01-01\",\"2014-01-01\",\"2015-01-01\",\"2016-01-01\",\"2017-01-01\", \"2018-01-01\", \"2019-01-01\", \"2020-01-01\"]\n",
        "pred_df = pd.DataFrame(data=test_predict, index = dates, columns=[\"y_pred\"])\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "real = plt.plot(spain_df[\"Value\"], color=\"pink\", label=\"real\")\n",
        "predicted = plt.plot(pred_df[\"y_pred\"], color = \"b\", linestyle='--', label=\"predicted\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNmo2CFOSzFG"
      },
      "source": [
        "Our model gives us a MAE of 467,686.06 tonnes, which as we expected is worse compared to our ARIMA and SARIMA models. If we had more time to tune our model structure and hyperparameters, we would expect the LSTM to perform much better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikgjdl91SzFG"
      },
      "source": [
        "### 6.4: Facebook Prophet  \n",
        "\n",
        "**6.4.1: Data Preparation**  \n",
        "\n",
        "**6.4.2: Model Fitting**  \n",
        "\n",
        "**6.4.3: Performance Evaluation** \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE1k8zWkSzFG"
      },
      "source": [
        "## 7: Model Comparison and Selection  \n",
        "\n",
        "### 7.1: Model Performance Metrics Comparison  \n",
        "\n",
        "<br>\n",
        "\n",
        "### 7.2: Best Model Selection\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWK6tQiZSzFG"
      },
      "source": [
        "## 8: Forecasting and Visualization  \n",
        "\n",
        "### 8.1: Future Crop Production Forecast  \n",
        "\n",
        "<br>\n",
        "\n",
        "### 8.2: Visualization of Forecasted Values and Confidence Intervals \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8qwELiiSzFG"
      },
      "source": [
        "## 9: Conclusion and Future Work  \n",
        "\n",
        "### 9.1: Summary of Findings  \n",
        "\n",
        "<br>\n",
        "\n",
        "### 9.2: Recommendations  \n",
        "\n",
        "<br>\n",
        "\n",
        "### 9.3: Future Research Directions \n",
        "\n",
        "In the future we would like to explore the important wine productors in the world like Italy, France and the United States to see if they behave similarly and if models created for those countries perform better than the one, we created for Spain.  \n",
        "\n",
        "We would like to include exogenous variables in the analysis as they provide additional information that can help improve the accuracy and predictive power of a model. Additionally, they can help you to identify causal relationships between variables. \n",
        "\n",
        "Lastly, we would like to investigate further the topic to understand the reasoning behind some trends we observed in the data.  \n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTIgIdMGSzFG"
      },
      "source": [
        "## 10: References \n",
        "- Bajaj, A. (2023, January 26). Arima & Sarima: Real-world time series forecasting. neptune.ai. Retrieved March 22, 2023, from https://neptune.ai/blog/arima-sarima-real-world-time-series-forecasting-guide  \n",
        "- Bonaros, B. (2022, July 4). Time series decomposition in Python. Medium. Retrieved March 22, 2023, from https://towardsdatascience.com/time-series-decomposition-in-python-8acac385a5b2  \n",
        "- FOASTAT Crops and livestock products. Faostat. (n.d.). Retrieved March 22, 2023, from https://www.fao.org/faostat/en/#data/QCL \n",
        "- Hayes, A. (2023, January 5). Autoregressive integrated moving average (ARIMA) prediction model. Investopedia. Retrieved March 22, 2023, from https://www.investopedia.com/terms/a/autoregressive-integrated-moving-average-arima.asp  \n",
        "- Kalita, D. (2022, March 24). An overview on Long short term memory (lstm). Analytics Vidhya. Retrieved March 22, 2023, from https://www.analyticsvidhya.com/blog/2022/03/an-overview-on-long-short-term-memory-lstm/  \n",
        "- Map of the month: How many people work in agriculture? Resource Watch Blog. (2020, May 4). Retrieved March 22, 2023, from https://blog.resourcewatch.org/2019/05/30/map-of-the-month-how-many-people-work-in-agriculture/#:~:text=The%20chart%20below%20shows%20data,down%20from%2044%25%20in%201991.  \n",
        "- Quick start. Prophet. (2023, February 28). Retrieved March 22, 2023, from https://facebook.github.io/prophet/docs/quick_start.html \n",
        "- El Niño Southern Oscillation (ENSO) region sea surface temperature forecasts. https://www.metoffice.gov.uk/research/climate/seasonal-to-decadal/gpc-outlooks/el-nino-la-nina#:~:text=Some%20regions%20have%20impacts%20that,the%20text%20on%20the%20map. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}